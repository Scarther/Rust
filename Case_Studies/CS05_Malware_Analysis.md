# Case Study 05: Static Malware Analysis Framework

## Scenario

Build a static malware analysis framework in Rust. The tool should parse binary files, extract indicators, and generate analysis reports without executing the malware.

---

## Learning Objectives

- Parse PE and ELF binary formats
- Extract strings and embedded data
- Calculate entropy for packer detection
- Identify suspicious characteristics
- Generate comprehensive analysis reports

---

## Phase 1: Binary Parser Foundation

### File Type Detection

```rust
use std::fs::File;
use std::io::Read;

#[derive(Debug)]
enum BinaryType {
    PE,       // Windows executable
    ELF,      // Linux executable
    MachO,    // macOS executable
    Script,   // Text-based script
    Unknown,
}

fn detect_file_type(path: &str) -> Result<BinaryType, String> {
    let mut file = File::open(path).map_err(|e| e.to_string())?;
    let mut magic = [0u8; 4];
    file.read_exact(&mut magic).map_err(|e| e.to_string())?;

    match magic {
        [0x4D, 0x5A, _, _] => Ok(BinaryType::PE),           // MZ header
        [0x7F, 0x45, 0x4C, 0x46] => Ok(BinaryType::ELF),    // ELF header
        [0xCF, 0xFA, 0xED, 0xFE] => Ok(BinaryType::MachO),  // Mach-O 64-bit
        [0xFE, 0xED, 0xFA, 0xCE] => Ok(BinaryType::MachO),  // Mach-O 32-bit
        [0x23, 0x21, _, _] => Ok(BinaryType::Script),       // Shebang #!
        _ => Ok(BinaryType::Unknown),
    }
}

fn main() {
    let test_files = vec![
        "/bin/ls",
        "/usr/bin/python3",
    ];

    for file in test_files {
        match detect_file_type(file) {
            Ok(file_type) => println!("{}: {:?}", file, file_type),
            Err(e) => println!("{}: Error - {}", file, e),
        }
    }
}
```

---

## Phase 2: String Extraction

### ASCII and Unicode String Extractor

```rust
use std::fs::File;
use std::io::Read;
use regex::Regex;

#[derive(Debug)]
struct ExtractedString {
    value: String,
    offset: usize,
    string_type: StringType,
    category: StringCategory,
}

#[derive(Debug)]
enum StringType {
    Ascii,
    Unicode,
}

#[derive(Debug)]
enum StringCategory {
    Url,
    IpAddress,
    FilePath,
    RegistryKey,
    Email,
    Command,
    Encoded,
    Generic,
}

fn categorize_string(s: &str) -> StringCategory {
    let patterns = vec![
        (Regex::new(r"https?://").unwrap(), StringCategory::Url),
        (Regex::new(r"\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}").unwrap(), StringCategory::IpAddress),
        (Regex::new(r"(?i)[a-z]:\\").unwrap(), StringCategory::FilePath),
        (Regex::new(r"(?i)HKEY_").unwrap(), StringCategory::RegistryKey),
        (Regex::new(r"@.*\.[a-z]{2,}").unwrap(), StringCategory::Email),
        (Regex::new(r"(?i)(cmd|powershell|wget|curl)").unwrap(), StringCategory::Command),
        (Regex::new(r"^[A-Za-z0-9+/]{20,}=*$").unwrap(), StringCategory::Encoded),
    ];

    for (pattern, category) in patterns {
        if pattern.is_match(s) {
            return category;
        }
    }

    StringCategory::Generic
}

fn extract_ascii_strings(data: &[u8], min_length: usize) -> Vec<ExtractedString> {
    let mut strings = Vec::new();
    let mut current = String::new();
    let mut start_offset = 0;

    for (i, &byte) in data.iter().enumerate() {
        if byte >= 0x20 && byte < 0x7F {
            if current.is_empty() {
                start_offset = i;
            }
            current.push(byte as char);
        } else {
            if current.len() >= min_length {
                let category = categorize_string(&current);
                strings.push(ExtractedString {
                    value: current.clone(),
                    offset: start_offset,
                    string_type: StringType::Ascii,
                    category,
                });
            }
            current.clear();
        }
    }

    strings
}

fn extract_unicode_strings(data: &[u8], min_length: usize) -> Vec<ExtractedString> {
    let mut strings = Vec::new();
    let mut current = String::new();
    let mut start_offset = 0;

    let mut i = 0;
    while i < data.len() - 1 {
        let byte = data[i];
        let next = data[i + 1];

        // UTF-16 LE: ASCII char followed by null
        if byte >= 0x20 && byte < 0x7F && next == 0x00 {
            if current.is_empty() {
                start_offset = i;
            }
            current.push(byte as char);
            i += 2;
        } else {
            if current.len() >= min_length {
                let category = categorize_string(&current);
                strings.push(ExtractedString {
                    value: current.clone(),
                    offset: start_offset,
                    string_type: StringType::Unicode,
                    category,
                });
            }
            current.clear();
            i += 1;
        }
    }

    strings
}

fn main() {
    let sample_data = b"Hello World\x00https://evil.com\x00192.168.1.1\x00cmd.exe /c calc\x00";

    let ascii_strings = extract_ascii_strings(sample_data, 4);
    let unicode_strings = extract_unicode_strings(sample_data, 4);

    println!("=== Extracted Strings ===\n");

    println!("ASCII Strings:");
    for s in &ascii_strings {
        println!("  [{:?}] {} @ 0x{:x}", s.category, s.value, s.offset);
    }

    println!("\nUnicode Strings:");
    for s in &unicode_strings {
        println!("  [{:?}] {} @ 0x{:x}", s.category, s.value, s.offset);
    }
}
```

---

## Phase 3: Entropy Analysis

### Packer Detection via Entropy

```rust
use std::collections::HashMap;
use std::fs::File;
use std::io::Read;

fn calculate_entropy(data: &[u8]) -> f64 {
    if data.is_empty() {
        return 0.0;
    }

    let mut freq: HashMap<u8, usize> = HashMap::new();
    for byte in data {
        *freq.entry(*byte).or_insert(0) += 1;
    }

    let len = data.len() as f64;
    let mut entropy = 0.0;

    for count in freq.values() {
        let p = *count as f64 / len;
        if p > 0.0 {
            entropy -= p * p.log2();
        }
    }

    entropy
}

fn calculate_section_entropy(data: &[u8], section_size: usize) -> Vec<(usize, f64)> {
    let mut results = Vec::new();

    for (i, chunk) in data.chunks(section_size).enumerate() {
        let offset = i * section_size;
        let entropy = calculate_entropy(chunk);
        results.push((offset, entropy));
    }

    results
}

fn interpret_entropy(entropy: f64) -> &'static str {
    match entropy {
        e if e < 1.0 => "Very low (mostly zeros/uniform)",
        e if e < 4.0 => "Low (plaintext/code)",
        e if e < 6.0 => "Medium (mixed content)",
        e if e < 7.5 => "High (compressed/encoded)",
        _ => "Very high (encrypted/packed)",
    }
}

fn main() {
    // Test with different data types
    let plaintext = b"The quick brown fox jumps over the lazy dog. ".repeat(10);
    let random_data: Vec<u8> = (0..1000).map(|i| (i * 17 + 31) as u8).collect();

    println!("=== Entropy Analysis ===\n");

    let e1 = calculate_entropy(&plaintext);
    println!("Plaintext entropy: {:.4} - {}", e1, interpret_entropy(e1));

    let e2 = calculate_entropy(&random_data);
    println!("Random data entropy: {:.4} - {}", e2, interpret_entropy(e2));

    // Section-by-section analysis
    println!("\nSection Analysis (256-byte chunks):");
    let mixed_data: Vec<u8> = plaintext.iter()
        .chain(random_data.iter())
        .copied()
        .collect();

    for (offset, entropy) in calculate_section_entropy(&mixed_data, 256).iter().take(10) {
        let bar_len = (entropy * 5.0) as usize;
        let bar: String = "#".repeat(bar_len);
        println!("  0x{:04x}: {:.4} {}", offset, entropy, bar);
    }
}
```

---

## Phase 4: PE Analysis

### Basic PE Parser

```rust
use std::fs::File;
use std::io::{Read, Seek, SeekFrom};

#[derive(Debug, Default)]
struct PEInfo {
    is_valid: bool,
    machine_type: String,
    timestamp: u32,
    sections: Vec<PESection>,
    imports: Vec<ImportEntry>,
    exports: Vec<String>,
    suspicious_indicators: Vec<String>,
}

#[derive(Debug)]
struct PESection {
    name: String,
    virtual_address: u32,
    virtual_size: u32,
    raw_size: u32,
    entropy: f64,
    characteristics: u32,
}

#[derive(Debug)]
struct ImportEntry {
    dll_name: String,
    functions: Vec<String>,
}

fn read_u16(data: &[u8], offset: usize) -> u16 {
    u16::from_le_bytes([data[offset], data[offset + 1]])
}

fn read_u32(data: &[u8], offset: usize) -> u32 {
    u32::from_le_bytes([
        data[offset],
        data[offset + 1],
        data[offset + 2],
        data[offset + 3],
    ])
}

fn analyze_pe(path: &str) -> Result<PEInfo, String> {
    let mut file = File::open(path).map_err(|e| e.to_string())?;
    let mut data = Vec::new();
    file.read_to_end(&mut data).map_err(|e| e.to_string())?;

    let mut info = PEInfo::default();

    // Check MZ header
    if data.len() < 64 || data[0] != 0x4D || data[1] != 0x5A {
        return Err("Not a valid PE file".to_string());
    }

    // Get PE header offset
    let pe_offset = read_u32(&data, 0x3C) as usize;

    // Check PE signature
    if data.len() < pe_offset + 4 {
        return Err("Invalid PE offset".to_string());
    }

    if &data[pe_offset..pe_offset + 4] != b"PE\x00\x00" {
        return Err("Invalid PE signature".to_string());
    }

    info.is_valid = true;

    // Parse COFF header
    let coff_offset = pe_offset + 4;
    let machine = read_u16(&data, coff_offset);
    info.machine_type = match machine {
        0x14c => "i386".to_string(),
        0x8664 => "AMD64".to_string(),
        _ => format!("Unknown (0x{:x})", machine),
    };

    info.timestamp = read_u32(&data, coff_offset + 4);

    let num_sections = read_u16(&data, coff_offset + 2) as usize;
    let optional_header_size = read_u16(&data, coff_offset + 16) as usize;

    // Parse sections
    let section_table_offset = coff_offset + 20 + optional_header_size;

    for i in 0..num_sections {
        let section_offset = section_table_offset + (i * 40);

        if section_offset + 40 > data.len() {
            break;
        }

        let name_bytes = &data[section_offset..section_offset + 8];
        let name = String::from_utf8_lossy(name_bytes)
            .trim_end_matches('\0')
            .to_string();

        let virtual_size = read_u32(&data, section_offset + 8);
        let virtual_address = read_u32(&data, section_offset + 12);
        let raw_size = read_u32(&data, section_offset + 16);
        let raw_offset = read_u32(&data, section_offset + 20) as usize;
        let characteristics = read_u32(&data, section_offset + 36);

        // Calculate section entropy
        let section_data = if raw_offset + raw_size as usize <= data.len() {
            &data[raw_offset..raw_offset + raw_size as usize]
        } else {
            &[]
        };
        let entropy = calculate_entropy(section_data);

        info.sections.push(PESection {
            name,
            virtual_address,
            virtual_size,
            raw_size,
            entropy,
            characteristics,
        });
    }

    // Check for suspicious indicators
    for section in &info.sections {
        if section.entropy > 7.0 {
            info.suspicious_indicators.push(format!(
                "High entropy section '{}': {:.2}",
                section.name, section.entropy
            ));
        }

        // Executable and writable section
        if section.characteristics & 0x20000000 != 0 && section.characteristics & 0x80000000 != 0 {
            info.suspicious_indicators.push(format!(
                "Section '{}' is both executable and writable",
                section.name
            ));
        }
    }

    Ok(info)
}

fn calculate_entropy(data: &[u8]) -> f64 {
    if data.is_empty() {
        return 0.0;
    }

    let mut freq = [0usize; 256];
    for byte in data {
        freq[*byte as usize] += 1;
    }

    let len = data.len() as f64;
    let mut entropy = 0.0;

    for count in freq.iter() {
        if *count > 0 {
            let p = *count as f64 / len;
            entropy -= p * p.log2();
        }
    }

    entropy
}

fn main() {
    let test_file = "/bin/ls";  // Or any PE file on Windows

    match analyze_pe(test_file) {
        Ok(info) => {
            println!("=== PE Analysis: {} ===\n", test_file);
            println!("Valid: {}", info.is_valid);
            println!("Machine: {}", info.machine_type);
            println!("Timestamp: {}", info.timestamp);

            println!("\nSections:");
            for section in &info.sections {
                println!("  {} - VA: 0x{:x}, Size: {}, Entropy: {:.2}",
                    section.name,
                    section.virtual_address,
                    section.raw_size,
                    section.entropy
                );
            }

            if !info.suspicious_indicators.is_empty() {
                println!("\nSuspicious Indicators:");
                for indicator in &info.suspicious_indicators {
                    println!("  [!] {}", indicator);
                }
            }
        }
        Err(e) => println!("Error: {}", e),
    }
}
```

---

## Phase 5: Complete Analysis Framework

```rust
use std::fs::{self, File};
use std::io::Read;
use chrono::Local;
use serde::{Serialize, Deserialize};

#[derive(Serialize, Deserialize)]
struct AnalysisReport {
    filename: String,
    analysis_time: String,
    file_size: u64,
    file_type: String,
    hashes: FileHashes,
    strings_summary: StringsSummary,
    entropy: EntropyAnalysis,
    indicators: Vec<String>,
    verdict: Verdict,
}

#[derive(Serialize, Deserialize)]
struct FileHashes {
    md5: String,
    sha1: String,
    sha256: String,
}

#[derive(Serialize, Deserialize)]
struct StringsSummary {
    total_count: usize,
    urls: Vec<String>,
    ips: Vec<String>,
    emails: Vec<String>,
    suspicious: Vec<String>,
}

#[derive(Serialize, Deserialize)]
struct EntropyAnalysis {
    overall: f64,
    interpretation: String,
    high_entropy_regions: Vec<String>,
}

#[derive(Serialize, Deserialize)]
enum Verdict {
    Clean,
    Suspicious,
    Malicious,
}

// Full implementation combines all phases
// See Projects/MalwareAnalyzer for complete code

fn main() {
    println!("=== Malware Analysis Framework ===\n");
    println!("Use: cargo run -- <sample_path>");
}
```

---

## Key Takeaways

1. **Never execute samples** - Static analysis only
2. **Multiple indicators** - No single check is definitive
3. **Entropy is key** - High entropy suggests packing/encryption
4. **String analysis** - Often reveals C2, paths, commands
5. **Document everything** - Reproducible analysis

---

## Exercises

1. Add YARA rule matching
2. Implement import table analysis
3. Add ELF support
4. Create VirusTotal API integration

---

[‚Üê Back to Case Studies](./README.md)
